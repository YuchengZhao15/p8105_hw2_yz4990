    library(tidyverse)

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

    library(readxl)

## Problem 2

read and clean the dataset from “Mr. Trash Wheel”, change sports\_balls
to integer

    mr_df = read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                       sheet = "Mr. Trash Wheel", 
                       range = "A2:N653", 
                       na = c("", "NA")) |> 
      janitor::clean_names() |> 
      mutate(
        sports_balls = as.integer(round(sports_balls, 0))
      ) |> 
      mutate(
        trash_wheel_name = "mr",
        year = as.numeric(year),
        month = tolower(month)
      )

read and clean the datasets from “Professor Trash Wheel” and “Gwynnda
Trash Wheel”

    prof_df = read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                       sheet = "Professor Trash Wheel", 
                       range = "A2:M121", 
                       na = c("", "NA")) |> 
      janitor::clean_names() |> 
      mutate(
        trash_wheel_name = "professor",
        month = tolower(month)
      )

    gw_df = read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                       sheet = "Gwynnda Trash Wheel", 
                       range = "A2:L265", 
                       na = c("", "NA")) |> 
      janitor::clean_names() |> 
      mutate(
        trash_wheel_name = "gwynnda",
        month = tolower(month)
      )

combine the 3 datasets with the new variable “trash\_wheel\_name”

    trash_wheel = 
      bind_rows(mr_df, prof_df, gw_df)

calculate the total weight and number of cigarette butts

    total_weight_prof = filter(trash_wheel, trash_wheel_name == "professor") |> 
      pull(weight_tons) |> 
      sum(na.rm = TRUE)

    june22_cig_gw = filter(trash_wheel, trash_wheel_name == "gwynnda", year == 2022, month == "june") |> 
      pull(cigarette_butts) |> 
      sum(na.rm = TRUE)

The 3 dataset from “Mr. Trash Wheel,” “Professor Trash Wheel,” “Gwynnda
Trash Wheel” each has 651, 119, and 263 observations respectively. The
combined dataset “trash\_wheel” has 1033 observations. Its key variables
include the date (“date”), weight of trash in tons (“weight\_tons”),
volume of trash in cubic yards (“volume\_cubic\_yards”), and the type of
trash (including “plastic\_bottles,” “cigarette\_butts,”
“glass\_bottles,” etc). The total weight of trash collected by Professor
Trash Wheel is 246.74 tons and the total number of cigarette butts
collected by Gwynnda in June of 2022 is 1.812^{4}.

## Problem 3

read and clean the 3 datasets

    bakers_df = read_csv("./gbb_datasets/bakers.csv",
                         na = c("", "NA", "N/A") ) |> 
      janitor::clean_names() |> 
      rename(
        baker = baker_name
      )

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

    bakes_df = read_csv("./gbb_datasets/bakes.csv",
                        na = c("", "NA", "N/A")) |> 
      janitor::clean_names()

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

    results_df = read_csv("./gbb_datasets/results.csv",
                          na = c("", "NA", "N/A"), 
                          skip = 2) |> 
      janitor::clean_names()

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

using anti\_join() to check for completeness and correctness

    anti_join(bakes_df, bakers_df, by = c("baker", "series"))

    ## # A tibble: 548 × 5
    ##    series episode baker     signature_bake                          show_stopper
    ##     <dbl>   <dbl> <chr>     <chr>                                   <chr>       
    ##  1      1       1 Annetha   "Light Jamaican Black Cakewith Strawbe… Red, White …
    ##  2      1       1 David     "Chocolate Orange Cake"                 Black Fores…
    ##  3      1       1 Edd       "Caramel Cinnamon and Banana Cake"      <NA>        
    ##  4      1       1 Jasminder "Fresh Mango and Passion Fruit Humming… <NA>        
    ##  5      1       1 Jonathan  "Carrot Cake with Lime and Cream Chees… Three Tiere…
    ##  6      1       1 Lea       "Cranberry and Pistachio Cakewith Oran… Raspberries…
    ##  7      1       1 Louise    "Carrot and Orange Cake"                Never Fail …
    ##  8      1       1 Mark      "Sticky Marmalade Tea Loaf"             Heart-shape…
    ##  9      1       1 Miranda   "Triple Layered Brownie Meringue Cake\… Three Tiere…
    ## 10      1       1 Ruth      "Three Tiered Lemon Drizzle Cakewith F… Classic Cho…
    ## # ℹ 538 more rows

    anti_join(results_df, bakers_df, by = c("baker", "series"))

    ## # A tibble: 1,136 × 5
    ##    series episode baker     technical result
    ##     <dbl>   <dbl> <chr>         <dbl> <chr> 
    ##  1      1       1 Annetha           2 IN    
    ##  2      1       1 David             3 IN    
    ##  3      1       1 Edd               1 IN    
    ##  4      1       1 Jasminder        NA IN    
    ##  5      1       1 Jonathan          9 IN    
    ##  6      1       1 Louise           NA IN    
    ##  7      1       1 Miranda           8 IN    
    ##  8      1       1 Ruth             NA IN    
    ##  9      1       1 Lea              10 OUT   
    ## 10      1       1 Mark             NA OUT   
    ## # ℹ 1,126 more rows

    anti_join(results_df, bakes_df, by = c("baker", "episode", "series"))

    ## # A tibble: 596 × 5
    ##    series episode baker    technical result
    ##     <dbl>   <dbl> <chr>        <dbl> <chr> 
    ##  1      1       2 Lea             NA <NA>  
    ##  2      1       2 Mark            NA <NA>  
    ##  3      1       3 Annetha         NA <NA>  
    ##  4      1       3 Lea             NA <NA>  
    ##  5      1       3 Louise          NA <NA>  
    ##  6      1       3 Mark            NA <NA>  
    ##  7      1       4 Annetha         NA <NA>  
    ##  8      1       4 Jonathan        NA <NA>  
    ##  9      1       4 Lea             NA <NA>  
    ## 10      1       4 Louise          NA <NA>  
    ## # ℹ 586 more rows

keep only the first name of bakers in bakers\_df, combine and organize
the datasets, export the final dataset

    bakers_df = mutate(bakers_df, 
                       baker = word(baker, 1))

    gbbo_df = full_join(bakes_df, bakers_df, by = c("baker", "series")) |> 
      full_join(results_df, by = c("baker", "episode", "series")) |> 
      select(series, episode, baker, baker_age, baker_occupation, signature_bake, technical, hometown, show_stopper, result)

    write_csv(gbbo_df, "./gbb_datasets/gbbo_df")

Describe your data cleaning process, including any questions you have or
choices you made. Briefly discuss the final dataset.

I first cleaned each of the 3 datasets
